---
title: "The Bug That Only Happened on Tuesdays"
date: "2024-01-10"
tags: ["debugging", "production", "distributed-systems"]
slug: "placeholder-debugging"
excerpt: "Payment processing failed every Tuesday at 3am. The logs showed nothing. After three weeks of dead ends, we finally found the culprit—and it wasn't in our code."
placeholder: true
---

## The Problem

Payment processing was failing every Tuesday at 3am UTC. Exactly Tuesday. Exactly 3am. Anywhere from 5 to 50 transactions would fail with generic timeout errors, then everything would work fine again.

## What Was at Stake

Each failure meant a frustrated customer who had to retry their purchase. Some didn't bother. We estimated $50K in lost revenue over the month. Worse, the pattern was eroding trust—customers were starting to avoid purchasing on certain days.

## The Constraint

We couldn't reproduce the bug in staging. Tuesday 3am in production was our only window, and we couldn't deliberately cause customer transactions to fail. We had to debug a live system without access to the failure conditions.

## The Key Moves

First, we instrumented everything. Added distributed tracing through the entire payment flow. Captured network latency at every hop. Logged connection pool states every 30 seconds.

```bash
# Added to cron: monitor connection states
*/1 * * * * pg_stat_activity --format=json >> /var/log/conn_state.log
*/1 * * * * redis-cli info clients >> /var/log/redis_clients.log
*/1 * * * * netstat -an | grep ESTABLISHED | wc -l >> /var/log/connections.log
```

Three Tuesdays of data collection revealed the pattern: database connections spiked at 2:58am, saturated the pool, then recovered. But nothing in our code ran at that time.

We traced the spike to our analytics service—a separate system that ran batch aggregations. It didn't talk to payments directly, but both services shared a database connection pooler.

The analytics batch exhausted the shared pool, starving payment transactions of connections.

## The Result

We separated the connection pools—dedicated allocations for each service class. Payment processing hasn't failed since.

The fix took 30 minutes. The investigation took three weeks. That ratio—weeks of debugging for minutes of coding—is more common than anyone admits.

We documented the postmortem extensively, not to assign blame, but to prevent recurrence. The analytics team didn't know their batch job affected other services. Now they do. We added alerts for connection pool saturation across all services.

## The Lesson

The bug wasn't in our code. It wasn't even in our system. It was in the infrastructure we shared with another team. When debugging intermittent issues, expand your blast radius. The cause is often outside your immediate codebase. Sometimes you need to step back and ask: "What else is happening at this exact time?"

I now start every debugging session by mapping the full dependency graph—not just code dependencies, but infrastructure dependencies. Shared resources are shared failure modes. The system is always bigger than the code you can see.
